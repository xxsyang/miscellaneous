{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  State Action  Transition Probability Next State  Reward\n",
      "0     a      x                    0.50          b       0\n",
      "1     a      x                    0.50          c       0\n",
      "2     a      y                    0.25          b       0\n",
      "3     a      y                    0.75          c       0\n",
      "4     b      x                    1.00          e       3\n",
      "5     b      y                    0.50          e       6\n",
      "6     b      y                    0.50          e       2\n",
      "7     c      x                    1.00          f       8\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Setting up the initial values and transitions based on the MDP structure provided in the image.\n",
    "# Here is an initial setup for each state, action, transition probabilities, and rewards.\n",
    "\n",
    "# Constants\n",
    "gamma = 1.0  # Discount factor\n",
    "\n",
    "# Initializing a dataframe to track the information for each state and action pair\n",
    "# Columns: State, Action, Transition Probability, Next State, Reward\n",
    "data = {\n",
    "    'State': ['a', 'a', 'a', 'a', 'b', 'b', 'b', 'c' ],\n",
    "    'Action': ['x', 'x', 'y', 'y', 'x', 'y', 'y', 'x'],\n",
    "    'Transition Probability': [0.5, 0.5, 0.25, 0.75, 1.0, 0.5, 0.5, 1.0],\n",
    "    'Next State': ['b', 'c', 'b', 'c', 'e', 'e', 'e', 'f'],\n",
    "    'Reward': [0, 0, 0, 0, 3, 6, 2, 8]\n",
    "}\n",
    "\n",
    "mdp_df = pd.DataFrame(data)\n",
    "\n",
    "print(mdp_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: {'a': 0.0, 'b': 3.0, 'c': 8.0, 'e': 0, 'f': 0}\n",
      "Iteration 2: {'a': 5.5, 'b': 3.0, 'c': 8.0, 'e': 0, 'f': 0}\n",
      "Iteration 3: {'a': 5.5, 'b': 3.0, 'c': 8.0, 'e': 0, 'f': 0}\n",
      "Iteration 4: {'a': 5.5, 'b': 3.0, 'c': 8.0, 'e': 0, 'f': 0}\n",
      "Iteration 5: {'a': 5.5, 'b': 3.0, 'c': 8.0, 'e': 0, 'f': 0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'a': 5.5, 'b': 3.0, 'c': 8.0, 'e': 0, 'f': 0}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize a dictionary to store the state values for the policy that always chooses action 'x'\n",
    "state_values_policy_x = {'a': 0, 'b': 0, 'c': 0, 'e': 0, 'f': 0}\n",
    "\n",
    "# Number of iterations for value iteration to converge (as gamma=1, we might need a few rounds)\n",
    "iterations = 5\n",
    "\n",
    "# Performing value iteration under policy where action 'x' is always chosen\n",
    "for iteration in range(iterations):\n",
    "    new_values = state_values_policy_x.copy()  # To store updated values in each iteration\n",
    "    for state in ['a', 'b', 'c']:\n",
    "        # Filter for rows where the current state and action is 'x'\n",
    "        state_action_x = mdp_df[(mdp_df['State'] == state) & (mdp_df['Action'] == 'x')]\n",
    "        \n",
    "        # Calculating expected value for the state under action 'x'\n",
    "        value = 0\n",
    "        for _, row in state_action_x.iterrows():\n",
    "            prob = row['Transition Probability']\n",
    "            next_state = row['Next State']\n",
    "            reward = row['Reward']\n",
    "            value += prob * (reward + gamma * state_values_policy_x[next_state])\n",
    "        \n",
    "        # Updating the new value for the state\n",
    "        new_values[state] = value\n",
    "    \n",
    "    # Updating state values after each iteration\n",
    "    state_values_policy_x = new_values\n",
    "\n",
    "    print(f\"Iteration {iteration + 1}: {state_values_policy_x}\")\n",
    "\n",
    "\n",
    "state_values_policy_x\n",
    "\n",
    "# {'a': 5.5, 'b': 3.0, 'c': 8.0, 'e': 0, 'f': 0}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 7.0, 'b': 4.0, 'c': 8.0, 'e': 0, 'f': 0}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize a dictionary to store the optimal state values\n",
    "optimal_state_values = {'a': 0, 'b': 0, 'c': 0, 'e': 0, 'f': 0}\n",
    "\n",
    "\n",
    "iterations = 100  # Number of iterations for value iteration to converge\n",
    "# Performing value iteration to find optimal state values (considering both actions 'x' and 'y')\n",
    "for _ in range(iterations):\n",
    "    new_values = optimal_state_values.copy()  # To store updated values in each iteration\n",
    "    for state in ['a', 'b', 'c']:\n",
    "        # Filter for rows where the current state matches\n",
    "        state_actions = mdp_df[mdp_df['State'] == state]\n",
    "        \n",
    "        # Calculate the value for each action and take the maximum\n",
    "        action_values = []\n",
    "        for action in ['x', 'y']:\n",
    "            action_df = state_actions[state_actions['Action'] == action]\n",
    "            value = 0\n",
    "            for _, row in action_df.iterrows():\n",
    "                prob = row['Transition Probability']\n",
    "                next_state = row['Next State']\n",
    "                reward = row['Reward']\n",
    "                value += prob * (reward + gamma * optimal_state_values[next_state])\n",
    "            action_values.append(value)\n",
    "        \n",
    "        # Update with the maximum action value for the state (optimal value)\n",
    "        new_values[state] = max(action_values)\n",
    "    \n",
    "    # Update the state values after each iteration\n",
    "    optimal_state_values = new_values\n",
    "\n",
    "optimal_state_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('a', 'x'): 6.0,\n",
       " ('a', 'y'): 7.0,\n",
       " ('b', 'x'): 3.0,\n",
       " ('b', 'y'): 4.0,\n",
       " ('c', 'x'): 8.0,\n",
       " ('c', 'y'): 0}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize a dictionary to store optimal Q-values for each (state, action) pair\n",
    "optimal_q_values = {}\n",
    "\n",
    "# Calculating Q-values for each (state, action) pair\n",
    "for state in ['a', 'b', 'c']:\n",
    "    # Filter for rows where the current state matches\n",
    "    state_actions = mdp_df[mdp_df['State'] == state]\n",
    "    \n",
    "    for action in ['x', 'y']:\n",
    "        # Filter for the specific action\n",
    "        action_df = state_actions[state_actions['Action'] == action]\n",
    "        \n",
    "        # Calculate Q-value for the specific (state, action) pair\n",
    "        q_value = 0\n",
    "        for _, row in action_df.iterrows():\n",
    "            prob = row['Transition Probability']\n",
    "            next_state = row['Next State']\n",
    "            reward = row['Reward']\n",
    "            q_value += prob * (reward + gamma * optimal_state_values[next_state])\n",
    "        \n",
    "        # Store the Q-value\n",
    "        optimal_q_values[(state, action)] = q_value\n",
    "\n",
    "optimal_q_values\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
